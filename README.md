---

# LLM-Conversational-RAG-App-LangChain-and-HuggingFace

---

This repository contains the implementation of a Conversational Retrieval-Augmented Generation (RAG) App using LangChain and the HuggingFace API. The app integrates large language models (LLMs) and document retrieval techniques to provide contextual and accurate responses by combining both pre-trained knowledge and custom user data.

## Features
- **Conversational Interface**: Provides an interactive environment for users to engage in meaningful and contextually aware conversations.
- **Retrieval-Augmented Generation (RAG)**: Enhances the modelâ€™s response by retrieving relevant information from a document store, improving accuracy.
- **LLM Integration**: Utilizes HuggingFace models for the underlying conversational language model.
- **Embeddings for Document Retrieval**: Efficiently retrieves relevant documents based on embeddings to ensure accurate context-aware responses.

## Models Used

### LLM Model
The app uses the [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) model from HuggingFace. This is a 7-billion-parameter model designed for high-quality conversational AI tasks, providing efficient and coherent responses.

### Embedding Model
For document retrieval, we use the [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model from HuggingFace. This model is known for generating high-quality embeddings that are effective for semantic search and retrieval tasks.

## How It Works

1. **User Input**: The user enters a question or a query into the app.
2. **Document Retrieval**: The app retrieves relevant documents from the document store using similarity search based on the embeddings generated by the embedding model.
3. **Response Generation**: The app combines the retrieved document with the user's input
